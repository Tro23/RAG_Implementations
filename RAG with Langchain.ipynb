{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67aeb6a0-9467-4772-b000-92e16facf7c2",
   "metadata": {},
   "source": [
    "### Installing the Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "386f2cfb-9978-4cfd-9d29-5f31924f1821",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q langchain\n",
    "!pip install -q torch\n",
    "!pip install -q transformers\n",
    "!pip install -q sentence-transformers\n",
    "!pip install -q datasets\n",
    "!pip install -q faiss-cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3daed92-223f-4353-bc61-c54036171e31",
   "metadata": {},
   "source": [
    "### Importing the Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0aa4a103-b876-4c08-89d0-e16ec37374c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rohit\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import HuggingFaceDatasetLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "from langchain import HuggingFacePipeline\n",
    "from langchain.chains import RetrievalQA\n",
    "import sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69e5ce4a-72a5-4c40-baeb-e0b2daf2c699",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb70df4d-1518-47b4-a11f-e6b26ca22ec6",
   "metadata": {},
   "source": [
    "### Document Loading \n",
    "        DataBricks Dolly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b83e848d-19b9-4b1b-8538-634df70ad6fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rohit\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\datasets\\load.py:2547: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.\n",
      "You can remove this warning by passing 'token=<use_auth_token>' instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"Virgin Australia, the trading name of Virgin Australia Airlines Pty Ltd, is an Australian-based airline. It is the largest airline by fleet size to use the Virgin brand. It commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route. It suddenly found itself as a major airline in Australia's domestic market after the collapse of Ansett Australia in September 2001. The airline has since grown to directly serve 32 cities in Australia, from hubs in Brisbane, Melbourne and Sydney.\", metadata={'instruction': 'When did Virgin Australia start operating?', 'response': 'Virgin Australia commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route.', 'category': 'closed_qa'}),\n",
       " Document(page_content='', metadata={'instruction': 'Which is a species of fish? Tope or Rope', 'response': 'Tope', 'category': 'classification'})]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Specify the dataset name and the column containing the content\n",
    "dataset_name = \"databricks/databricks-dolly-15k\"\n",
    "page_content_column = \"context\"  # or any other column you're interested in\n",
    "\n",
    "# Create a loader instance\n",
    "loader = HuggingFaceDatasetLoader(dataset_name, page_content_column)\n",
    "\n",
    "# Load the data\n",
    "data = loader.load()\n",
    "\n",
    "# Display the first 15 entries\n",
    "data[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db4a7bb-fc45-4c32-be3e-7f5a968ddf2b",
   "metadata": {},
   "source": [
    "### Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac5904b-8a33-46cf-b5a4-516a7f56b8c6",
   "metadata": {},
   "source": [
    "#### Text Splitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19d6e212-20ba-4a5b-9fbb-f1ed430b3ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the RecursiveCharacterTextSplitter class with specific parameters.\n",
    "# It splits text into chunks of 1000 characters each with a 150-character overlap.\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)\n",
    "\n",
    "# 'data' holds the text you want to split, split the text into documents using the text splitter.\n",
    "docs = text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3465339c-5c41-4029-816d-0215ab0e02ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content=\"Virgin Australia, the trading name of Virgin Australia Airlines Pty Ltd, is an Australian-based airline. It is the largest airline by fleet size to use the Virgin brand. It commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route. It suddenly found itself as a major airline in Australia's domestic market after the collapse of Ansett Australia in September 2001. The airline has since grown to directly serve 32 cities in Australia, from hubs in Brisbane, Melbourne and Sydney.\", metadata={'instruction': 'When did Virgin Australia start operating?', 'response': 'Virgin Australia commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route.', 'category': 'closed_qa'})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78fb76d-0e6e-46ca-80e5-f361be5fe1d3",
   "metadata": {},
   "source": [
    "### Text Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a823b5c-0ed9-4de0-ba3f-5aba6052a1fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rohit\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Using the HuggingFaceEmbeddings\n",
    "\n",
    "# Define the path to the pre-trained model you want to use\n",
    "modelPath = \"sentence-transformers/all-MiniLM-l6-v2\"\n",
    "\n",
    "# Create a dictionary with model configuration options, specifying to use the CPU for computations\n",
    "model_kwargs = {'device':'cpu'}\n",
    "\n",
    "# Create a dictionary with encoding options, specifically setting 'normalize_embeddings' to False\n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "\n",
    "# Initialize an instance of HuggingFaceEmbeddings with the specified parameters\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=modelPath,     # Provide the pre-trained model's path\n",
    "    model_kwargs=model_kwargs, # Pass the model configuration options\n",
    "    encode_kwargs=encode_kwargs # Pass the encoding options\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7609d148-3b04-4e07-8818-5b7db01e2cba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.03833853453397751, 0.1234646737575531, -0.02864297851920128]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"This is a test document.\"\n",
    "query_result = embeddings.embed_query(text)\n",
    "query_result[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b0956d-ff8e-4962-9250-6ba0c0769e53",
   "metadata": {},
   "source": [
    "#### Vector Stores\n",
    "        FAISS vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f154657-8c2f-4bd5-89f0-152069b8ef69",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = FAISS.from_documents(docs, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "289cbc50-26ba-4ff0-9def-2d1f5b25416d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The earliest references to peanut butter can be traced to Aztec and Inca civilizations, who ground roasted peanuts into a paste.[clarification needed] However, several people can be credited with the invention of modern peanut butter and the processes involved in making it.[according to whom?]\n",
      "\n",
      "The US National Peanut Board credits three modern inventors with the earliest patents related to the production of modern peanut butter. Marcellus Gilmore Edson of Montreal, Quebec, Canada, obtained the first patent for a method of producing peanut butter from roasted peanuts using heated surfaces in 1884. Edson's cooled product had \"a consistency like that of butter, lard, or ointment\" according to his patent application which described a process of milling roasted peanuts until the peanuts reached \"a fluid or semi-fluid state\". He mixed sugar into the paste to harden its consistency.[citation needed]\n"
     ]
    }
   ],
   "source": [
    "### Searching our Question\n",
    "\n",
    "question = \"What is butter making?\"\n",
    "searchDocs = db.similarity_search(question)\n",
    "print(searchDocs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f49c78-636d-49e7-bafa-bc3fd3bab7fa",
   "metadata": {},
   "source": [
    "### Preparing the LLM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b08753-9838-4bf1-87b1-2b09d44d0d17",
   "metadata": {},
   "source": [
    "#### Using Google/Pegasus-Large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "96570171-ec43-487c-8c8d-5f17ff94508f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rohit\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\rohit\\.cache\\huggingface\\hub\\models--google--pegasus-large. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\rohit\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-large and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Create a tokenizer object by loading the pretrained \"Intel/dynamic_tinybert\" tokenizer.\n",
    "tokenizer =  AutoTokenizer.from_pretrained(\"google/pegasus-large\")\n",
    "\n",
    "# Create a question-answering model object by loading the pretrained \"Intel/dynamic_tinybert\" model.\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/pegasus-large\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d813bb-de06-4e50-98c4-2eec508582bb",
   "metadata": {},
   "source": [
    "#### Creating a Question Answering Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1992b318-8dfe-44cb-87d3-b0ec2be46e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-large and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Specify the model name you want to use\n",
    "model_name = \"google/pegasus-large\"\n",
    "\n",
    "# Load the tokenizer associated with the specified model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, padding=True, truncation=True, max_length=512)\n",
    "\n",
    "# Define a question-answering pipeline using the model and tokenizer\n",
    "question_answerer = pipeline(\n",
    "    \"text2text-generation\", \n",
    "    model=model_name, \n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# Create an instance of the HuggingFacePipeline, which wraps the question-answering pipeline\n",
    "# with additional model-specific arguments (temperature and max_length)\n",
    "llm = HuggingFacePipeline(\n",
    "    pipeline=question_answerer,\n",
    "    model_kwargs={\"temperature\": 0.7, \"max_length\": 512},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27d827a-ebf7-4453-9052-91b1a6ed021c",
   "metadata": {},
   "source": [
    "#### Retrievers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8759d2c0-b24c-45f6-acea-a32a378dd54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a retriever object from the 'db' using the 'as_retriever' method.\n",
    "# This retriever is likely used for retrieving data or documents from the database.\n",
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "568bc068-9cdf-43ea-902a-6c6d52ed925a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The earliest references to peanut butter can be traced to Aztec and Inca civilizations, who ground roasted peanuts into a paste.[clarification needed] However, several people can be credited with the invention of modern peanut butter and the processes involved in making it.[according to whom?]\n",
      "\n",
      "The US National Peanut Board credits three modern inventors with the earliest patents related to the production of modern peanut butter. Marcellus Gilmore Edson of Montreal, Quebec, Canada, obtained the first patent for a method of producing peanut butter from roasted peanuts using heated surfaces in 1884. Edson's cooled product had \"a consistency like that of butter, lard, or ointment\" according to his patent application which described a process of milling roasted peanuts until the peanuts reached \"a fluid or semi-fluid state\". He mixed sugar into the paste to harden its consistency.[citation needed]\n"
     ]
    }
   ],
   "source": [
    "docs = retriever.get_relevant_documents(\"What is butter making?\")\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71213d6c-bc8e-4693-b656-68ce7138cf3a",
   "metadata": {},
   "source": [
    "#### Retrieval QA Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f900789d-5c55-4851-8214-c425cf2c26a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a retriever object from the 'db' with a search configuration where it retrieves up to 4 relevant splits/documents.\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": 4})\n",
    "\n",
    "# Create a question-answering instance (qa) using the RetrievalQA class.\n",
    "# It's configured with a language model (llm), a chain type \"refine,\" the retriever we created, and an option to not return source documents.\n",
    "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"refine\", retriever=retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d8a44f29-993f-466f-8661-e90e49116ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Who is Thomas Jefferson?\"\n",
    "result = qa.run({\"query\": \"Who is Thomas Jefferson?\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "34298771-1eb8-4f2f-a72a-f176867f23dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have provided an existing answer: ------------ Given the new context, refine the original answer to better answer the question.\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716572b6-ca19-43c3-a511-19e1ce63d63f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
